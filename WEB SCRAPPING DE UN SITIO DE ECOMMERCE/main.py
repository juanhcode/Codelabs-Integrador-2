# Importar librer√≠as necesarias
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin, urlparse
from datetime import datetime

print("‚úÖ Librer√≠as importadas correctamente")

# Configuraci√≥n del scraper
base_url = "http://books.toscrape.com"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

def get_page_content(url):
    """Obtiene el contenido de una p√°gina"""
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return BeautifulSoup(response.text, 'html.parser')
    except requests.RequestException as e:
        print(f"Error al acceder a {url}: {e}")
        return None

# Probar la conexi√≥n
print("üîÑ Probando conexi√≥n...")
soup = get_page_content(base_url)
if soup:
    print("‚úÖ Conexi√≥n exitosa")
    print(f"T√≠tulo de la p√°gina: {soup.title.text}")
else:
    print("‚ùå Error en la conexi√≥n")

# Analizar la primera p√°gina
if soup:
    # Buscar productos
    products = soup.find_all('article', class_='product_pod')
    print(f"üìö Encontrados {len(products)} productos en la primera p√°gina")

    # Analizar el primer producto
    if products:
        first_product = products[0]
        print(f"\nüîç An√°lisis del primer producto:")
        print(f"HTML del producto: {str(first_product)[:200]}...")

        # Buscar elementos espec√≠ficos
        title_elem = first_product.find('h3')
        if title_elem:
            title_link = title_elem.find('a')
            if title_link:
                print(f"T√≠tulo: {title_link.get('title', 'N/A')}")
                print(f"Enlace: {title_link.get('href', 'N/A')}")

        price_elem = first_product.find('p', class_='price_color')
        if price_elem:
            print(f"Precio: {price_elem.text}")

        rating_elem = first_product.find('p', class_='star-rating')
        if rating_elem:
            print(f"Rating: {rating_elem.get('class', 'N/A')}")

        availability_elem = first_product.find('p', class_='instock availability')
        if availability_elem:
            print(f"Disponibilidad: {availability_elem.text.strip()}")

    # Buscar enlace a la siguiente p√°gina
    next_link = soup.find('li', class_='next')
    if next_link:
        next_url = next_link.find('a')
        if next_url:
            print(f"\n‚û°Ô∏è Siguiente p√°gina: {next_url.get('href')}")
    else:
        print("\n‚ùå No se encontr√≥ enlace a la siguiente p√°gina")
else:
    print("‚ùå No se pudo analizar la p√°gina")

def extract_product_info(product_element):
    """Extrae informaci√≥n de un elemento de producto"""
    try:
        # T√≠tulo del libro
        title_element = product_element.find('h3').find('a')
        title = title_element.get('title', '').strip()

        # URL del producto
        product_url = title_element.get('href', '')
        product_url = urljoin(base_url, product_url)

        # Precio
        price_element = product_element.find('p', class_='price_color')
        price = price_element.text.strip() if price_element else 'N/A'

        # Rating (estrellas)
        rating_element = product_element.find('p', class_='star-rating')
        rating = rating_element.get('class')[1] if rating_element else 'No rating'

        # Disponibilidad
        availability_element = product_element.find('p', class_='instock availability')
        availability = availability_element.text.strip() if availability_element else 'N/A'

        return {
            'title': title,
            'price': price,
            'rating': rating,
            'availability': availability,
            'product_url': product_url,
            'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    except Exception as e:
        print(f"Error al extraer informaci√≥n del producto: {e}")
        return None

# Probar con el primer producto
if soup and products:
    print("üß™ Probando extracci√≥n del primer producto...")
    first_product_info = extract_product_info(products[0])
    if first_product_info:
        print("‚úÖ Informaci√≥n extra√≠da exitosamente:")
        for key, value in first_product_info.items():
            print(f"  {key}: {value}")
    else:
        print("‚ùå Error al extraer informaci√≥n")
else:
    print("‚ùå No hay productos para probar")

def scrape_all_books(max_pages=3):
    """Scraper completo que maneja m√∫ltiples p√°ginas"""
    all_books = []
    current_page = 1

    print(f"üöÄ Iniciando scraping de {max_pages} p√°ginas...")

    while current_page <= max_pages:
        if current_page == 1:
            url = base_url
        else:
            url = f"{base_url}/catalogue/page-{current_page}.html"

        print(f"üìÑ Procesando p√°gina {current_page}...")

        soup = get_page_content(url)
        if not soup:
            print(f"‚ùå Error en p√°gina {current_page}")
            break

        # Verificar si hay productos en la p√°gina
        products = soup.find_all('article', class_='product_pod')
        if not products:
            print(f"‚ùå No se encontraron productos en p√°gina {current_page}")
            break

        # Extraer informaci√≥n de cada producto
        page_books = []
        for product in products:
            book_info = extract_product_info(product)
            if book_info:
                book_info['page'] = current_page
                page_books.append(book_info)

        all_books.extend(page_books)
        print(f"  ‚úÖ Extra√≠dos {len(page_books)} libros de la p√°gina {current_page}")

        # Pausa entre peticiones para ser respetuosos
        time.sleep(1)
        current_page += 1

    print(f"\nüéâ Scraping completado!")
    print(f"üìä Total de libros extra√≠dos: {len(all_books)}")
    return all_books

# Ejecutar el scraper (limitado a 3 p√°ginas para el ejemplo)
books = scrape_all_books(max_pages=3)

def clean_and_analyze_books(books_data):
    """Limpia y analiza los datos de libros"""
    if not books_data:
        print("‚ùå No hay datos para analizar")
        return None

    df = pd.DataFrame(books_data)

    print("=== INFORMACI√ìN B√ÅSICA ===")
    print(f"Total de libros: {len(df)}")
    print(f"Columnas: {list(df.columns)}")

    # Limpiar precios
    def clean_price(price_str):
        if price_str == 'N/A':
            return 0.0
        # Extraer solo n√∫meros del precio
        price_clean = re.sub(r'[^\d.]', '', price_str)
        try:
            return float(price_clean)
        except:
            return 0.0

    df['price_numeric'] = df['price'].apply(clean_price)

    # An√°lisis de precios
    print("\n=== AN√ÅLISIS DE PRECIOS ===")
    valid_prices = df[df['price_numeric'] > 0]
    if len(valid_prices) > 0:
        print(f"Precio promedio: ¬£{valid_prices['price_numeric'].mean():.2f}")
        print(f"Precio m√≠nimo: ¬£{valid_prices['price_numeric'].min():.2f}")
        print(f"Precio m√°ximo: ¬£{valid_prices['price_numeric'].max():.2f}")

    # An√°lisis por rating
    print("\n=== AN√ÅLISIS POR RATING ===")
    rating_counts = df['rating'].value_counts()
    print(rating_counts)

    # Libros m√°s caros
    if len(valid_prices) > 0:
        print("\n=== TOP 5 LIBROS M√ÅS CAROS ===")
        expensive_books = valid_prices.nlargest(5, 'price_numeric')[['title', 'price', 'rating']]
        for idx, row in expensive_books.iterrows():
            print(f"- {row['title']} - {row['price']} ({row['rating']})")

    return df

# Analizar los datos
if books:
    df_books = clean_and_analyze_books(books)
else:
    print("‚ùå No hay datos para analizar")

def search_books_by_criteria(df, criteria):
    """Busca libros por criterios espec√≠ficos"""
    if df is None or df.empty:
        print("‚ùå No hay datos para buscar")
        return df

    results = df.copy()

    # Filtrar por t√≠tulo
    if 'title_keywords' in criteria:
        keywords = criteria['title_keywords'].lower().split()
        mask = results['title'].str.lower().str.contains('|'.join(keywords), na=False)
        results = results[mask]

    # Filtrar por precio
    if 'max_price' in criteria:
        results = results[results['price_numeric'] <= criteria['max_price']]

    if 'min_price' in criteria:
        results = results[results['price_numeric'] >= criteria['min_price']]

    # Filtrar por rating
    if 'min_rating' in criteria:
        rating_order = ['One', 'Two', 'Three', 'Four', 'Five']
        min_rating_idx = rating_order.index(criteria['min_rating'])
        valid_ratings = rating_order[min_rating_idx:]
        results = results[results['rating'].isin(valid_ratings)]

    return results

# Ejemplos de b√∫squeda
if 'df_books' in locals() and df_books is not None:
    print("=== B√öSQUEDAS ESPEC√çFICAS ===")

    # Buscar libros sobre "python"
    python_books = search_books_by_criteria(df_books, {'title_keywords': 'python'})
    print(f"Libros sobre Python: {len(python_books)}")

    # Buscar libros baratos (menos de ¬£10)
    cheap_books = search_books_by_criteria(df_books, {'max_price': 10})
    print(f"Libros baratos (<¬£10): {len(cheap_books)}")

    # Buscar libros con rating alto
    high_rated = search_books_by_criteria(df_books, {'min_rating': 'Four'})
    print(f"Libros con rating alto (4+ estrellas): {len(high_rated)}")

    # Mostrar algunos resultados
    if len(python_books) > 0:
        print(f"\nüêç Libros sobre Python encontrados:")
        for idx, row in python_books.head(3).iterrows():
            print(f"- {row['title']} - {row['price']}")

    if len(cheap_books) > 0:
        print(f"\nüí∞ Libros baratos encontrados:")
        for idx, row in cheap_books.head(3).iterrows():
            print(f"- {row['title']} - {row['price']}")
else:
    print("‚ùå No hay datos para buscar")

def save_books_data(df, base_filename='books_data'):
    """Guarda los datos en diferentes formatos"""
    if df is None or df.empty:
        print("‚ùå No hay datos para guardar")
        return

    # CSV
    csv_filename = f"{base_filename}.csv"
    df.to_csv(csv_filename, index=False, encoding='utf-8')
    print(f"‚úÖ Datos guardados en CSV: {csv_filename}")

    # Excel (si est√° disponible)
    try:
        excel_filename = f"{base_filename}.xlsx"
        df.to_excel(excel_filename, index=False)
        print(f"‚úÖ Datos guardados en Excel: {excel_filename}")
    except ImportError:
        print("‚ö†Ô∏è Excel no disponible (instalar openpyxl para soporte)")

    # JSON
    json_filename = f"{base_filename}.json"
    df.to_json(json_filename, orient='records', indent=2)
    print(f"‚úÖ Datos guardados en JSON: {json_filename}")

    # Resumen estad√≠stico
    summary_filename = f"{base_filename}_summary.txt"
    with open(summary_filename, 'w', encoding='utf-8') as f:
        f.write("=== RESUMEN DE DATOS ===\n")
        f.write(f"Total de libros: {len(df)}\n")

        valid_prices = df[df['price_numeric'] > 0]
        if len(valid_prices) > 0:
            f.write(f"Precio promedio: ¬£{valid_prices['price_numeric'].mean():.2f}\n")
            f.write(f"Precio m√≠nimo: ¬£{valid_prices['price_numeric'].min():.2f}\n")
            f.write(f"Precio m√°ximo: ¬£{valid_prices['price_numeric'].max():.2f}\n")

        f.write(f"\nDistribuci√≥n por rating:\n")
        f.write(df['rating'].value_counts().to_string())

    print(f"‚úÖ Resumen guardado en: {summary_filename}")

# Guardar todos los datos
if 'df_books' in locals() and df_books is not None:
    save_books_data(df_books)
else:
    print("‚ùå No hay datos para guardar")